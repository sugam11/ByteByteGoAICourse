{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0fb30d",
   "metadata": {},
   "source": [
    "# Project 4: **Build a Deep Research System**\n",
    "Welcome to project 4! For this project, we shift our focus from tool use and agents to *reasoning* models. You will practice state‚Äëof‚Äëthe‚Äëart inference‚Äëtime scaling methods such as *Chain‚Äëof‚ÄëThought* prompting and *Tree‚Äëof‚ÄëThoughts*, and briefly explore high-levels of training reasoning models using techniques like **STaR**.\n",
    "\n",
    "\n",
    "Finally, you will put everything together to build a *deep research agent* that can browse the web, reason over what it finds, and give structured answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54845369",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Apply common inference‚Äëtime scaling methods: **zero‚Äëshot / few‚Äëshot CoT, self‚Äëconsistency, sequential decoding, tree‚Äëof‚Äëthoughts**  \n",
    "* Gain intuition for **training** reasoning‚Äëcapable models following **STaR** approach \n",
    "* Build a minimal **deep‚Äëresearch agent** that combines step‚Äëby‚Äëstep reasoning with live web search   \n",
    "* Practice extending deep-search to a multi-agent system "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a40a86",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "1. Environment setup  \n",
    "2. Inference‚Äëtime scaling  \n",
    "   2.1 Few‚Äëshot & zero‚Äëshot‚ÄØCoT  \n",
    "   2.2 Self‚Äëconsistency\n",
    "   2.3 Sequential revisions  \n",
    "   2.4 Tree‚Äëof‚ÄëThought\n",
    "3. STaR for training models for reasoning  \n",
    "4. Deep-research agent  \n",
    "5. (Optional) Multi-agent deep-research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e480f76",
   "metadata": {},
   "source": [
    "# 1‚Äë Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c2218",
   "metadata": {},
   "source": [
    "## 1.1- Conda environment\n",
    "\n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook and run:\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yaml && conda activate deep_research\n",
    "\n",
    "# Register this environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=deep_research --display-name \"deep_research\"\n",
    "```\n",
    "Once this is done, you can select \"deep_research\" from the Kernel ‚Üí Change Kernel menu in Jupyter or VS Code.\n",
    "\n",
    "## 1.2 Ollama setup\n",
    "\n",
    "In this project we use the `llama3.2:3b` and `deepseek-r1:8b` models. You can try other smaller or larger reasoning LLMs such as `qwen2.5:3b-instruct` or `phi4-mini` to compare performance. Explore available models here: https://ollama.com/library.\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "ollama pull deepseek-r1:8b\n",
    "# Additional small reasoning models to compare\n",
    "# ollama pull qwen2.5:3b-instruct\n",
    "# ollama pull phi4-mini\n",
    "\n",
    "```\n",
    "\n",
    "`ollama pull` downloads the model so you can run it locally without API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8d1b7",
   "metadata": {},
   "source": [
    "---  \n",
    "# 2‚Äë Inference‚Äëtime scaling\n",
    "\n",
    "Inference-time scaling refers to techniques that make an existing model reason better without retraining it. Instead of changing the model‚Äôs weights, we achieve reasoning capability by adjusting how we prompt, sample, or aggregate LLM's outputs.\n",
    "\n",
    "In this section, we‚Äôll explore several inference-time strategies that improve reasoning quality using a non-reasoning base model. You will experiment with and compare methods such as:\n",
    "\n",
    "- Few-shot Chain-of-Thought (CoT)\n",
    "- Zero-shot CoT\n",
    "- Self-consistency\n",
    "- Sequential revision\n",
    "- Tree-of-Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d499a",
   "metadata": {},
   "source": [
    "### 2.1: Few‚ÄëShot CoT\n",
    "Few-shot prompting helps a model reason by showing one or multiple examples before asking a new question. By observing the pattern of reasoning and final answers, the model learns how to structure its own reasoning process on the new input.\n",
    "\n",
    "In this exercise, you will create a prompt that includes a few example Q&A pairs demonstrating step-by-step reasoning. Then, you will feed a new question and see the model‚Äôs output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "173d73f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine which one is closer to Earth, let's consider the average distances of each.\n",
      "\n",
      "Thinking Step:\n",
      "\n",
      "1. The average distance from the Earth to the Moon is about 384,400 kilometers (238,900 miles).\n",
      "2. The average distance from the Earth to the Sun is about 149,600,000 kilometers (92,960,000 miles).\n",
      "\n",
      "Now, let's compare these distances:\n",
      "\n",
      "149,600,000 km (Sun) > 384,400 km (Moon)\n",
      "\n",
      "Since the Moon is significantly closer to Earth than the Sun, we can conclude that the Moon is closer to our planet.\n",
      "\n",
      "A: The Moon\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Write a few examples showing reasoning steps\n",
    "# Step 2: Write your new question\n",
    "# Step 3: Concatenate examples + new question into a single prompt\n",
    "# Step 4: Call your Ollama or OpenAI client to get a response from llama3.2:3b # e.g., client.chat.completions.create(...)\n",
    "# Step 5: Print the final answer\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "cot_examples = [\n",
    "    \"Q: Who is older Obama or Trump?\\nThinking Step: Obama is 64 yrs old. Trump is 78 yrs old. 78 > 64, which means Trump is older.\\nA: Trump\",\n",
    "    \"Q: If I have a monthly salary of 10,000 and I spend 2000 on rent, 500 on groceries, 1000 on miscellaneous. How much do I save?\\nThinking Step: Total Expenses are 2000+500+1000 = 3500. Savings is income minus expenses.\\nA: 6500\"\n",
    "]\n",
    "\n",
    "question = \"Q: What is closer to earth - moon or sun?\"\n",
    "input_prompt = (\n",
    "    \"Below are some examples of how to reason step-by-step before giving an answer.\\n\"\n",
    "    \"Follow the same reasoning style to answer the final question only.\\n\\n\"\n",
    "    + \"\\n\\n\".join(cot_examples)\n",
    "    + f\"\\n\\nQ: {question}\"\n",
    ")\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "response = client.chat.completions.create(\n",
    "    model='llama3.2:3b',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_prompt\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e29d9",
   "metadata": {},
   "source": [
    "### (Optional) Few-shot CoT on GPT2\n",
    "GPT-2 is a pre-trained language model without instruction tuning. It continues text rather than answering questions. In this section, you'll try the exact same CoT pattern on GPT-2 and observe what happens. The goal is to test whether few-shot CoT alone can elicit structured reasoning from a non-chat LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Load GPT-2 text-generation from huggingface (https://huggingface.co/docs/transformers/en/model_doc/gpt2)\n",
    "# Step 2: Write 1‚Äì2 few-shot reasoning examples (short, explicit steps + final answer in your own unique format)\n",
    "# Step 3: Append a new test question after the examples to form one prompt string\n",
    "# Step 4: Generate 1‚Äì3 completions with different decoding settings (e.g., greedy vs. top-k)\n",
    "# Step 5: Print raw outputs; check if steps are followed and if the final answer is correct\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~12-15 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adee0e7",
   "metadata": {},
   "source": [
    "### 2.2: Zero‚ÄëShot Chain‚Äëof‚ÄëThought\n",
    "Zero-shot CoT encourages the model to reason without examples by adding a short cue such as ‚ÄúLet‚Äôs think step by step.‚Äù This simple phrase often activates the model‚Äôs latent reasoning ability even when no demonstrations are provided. It serves as a baseline to compare with few-shot and other inference-time scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c444eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The Moon.\n",
      "\n",
      "Reasoning:\n",
      "\n",
      "1. The average distance from Earth to the Sun is about 93 million miles (149.6 million kilometers), which is called an astronomical unit (AU).\n",
      "2. The average distance from Earth to the Moon is about 238,855 miles (384,400 kilometers).\n",
      "\n",
      "Comparing these two distances, we can see that the Moon is much closer to Earth than the Sun.\n",
      "\n",
      "To put it into perspective:\n",
      "\n",
      "* The closest point in the Moon's orbit to Earth is called perigee, which is approximately 225,622 miles (363,104 kilometers) away.\n",
      "* The farthest point from Earth is called apogee, which is about 252,088 miles (405,696 kilometers) away.\n",
      "\n",
      "Since the Moon's average distance is less than half of the Sun's average distance, it is indeed closer to Earth.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Step 1: Write the question and a zero-shot CoT cue (e.g., \"Let's think step by step.\")\n",
    "# Step 2: Build a single prompt string that includes brief role guidance plus the question\n",
    "# Step 3: Call your Ollama or OpenAI client to get a response from llama3.2:3b  # e.g., client.chat.completions.create(...)\n",
    "# Step 4: Print the chain and the final answer\n",
    "\n",
    "question = \"What is closer to earth - moon or sun?\"\n",
    "input_prompt = (\n",
    "    f\"Q: {question}\"\n",
    "    + \"\\n\\nLet's think step by step. Start the final answer with A: and reasoning with reasoning:\"\n",
    ")\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "response = client.chat.completions.create(\n",
    "    model='llama3.2:3b',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_prompt\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686708da",
   "metadata": {},
   "source": [
    "### 2.3 Self‚ÄëConsistency\n",
    "Self-consistency enhances reasoning accuracy by sampling multiple independent reasoning paths for the same question instead of relying on a single deterministic answer. Each run may follow a slightly different logical chain, and the diversity helps correct individual mistakes. After generating several reasoning traces, you then aggregate the final answers using majority voting.\n",
    "\n",
    "This approach is especially useful when tasks involve multi-step reasoning or arithmetic, where single-path outputs may be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e2fb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 12\n",
      "Answer: 12\n",
      "Answer: 12\n",
      "Answer: 12\n",
      "Answer: The square root of 144 is 12.\n",
      "Answer: 12\n",
      "Answer: $\\sqrt{144} = 12$\n",
      "Answer: 12\n",
      "Answer: 12\n",
      "Answer: 12\n",
      "Votes: Counter({'12': 8, 'The square root of 144 is 12.': 1, '$\\\\sqrt{144} = 12$': 1})\n",
      "Chosen answer: ['12']\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import re, collections\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_model_output(text):\n",
    "    \"\"\"\n",
    "    Extracts final answer and reasoning (if present) from model output.\n",
    "    Returns (answer, reasoning)\n",
    "    \"\"\"\n",
    "    # Try to match reasoning + answer\n",
    "    pattern_full = r\"Reasoning:\\s*(?P<reasoning>[\\s\\S]*?)\\nA:\\s*(?P<answer>.*)\"\n",
    "    match = re.search(pattern_full, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(\"answer\").strip(), match.group(\"reasoning\").strip()\n",
    "\n",
    "    # If reasoning not present, just extract answer\n",
    "    pattern_answer_only = r\"A:\\s*(?P<answer>.*)\"\n",
    "    match = re.search(pattern_answer_only, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(\"answer\").strip(), None\n",
    "\n",
    "    # Fallback\n",
    "    return None, None\n",
    "\n",
    "def cot_answer(question, temperature=1.0):\n",
    "    # Generate a step-by-step reasoning chain for the given question and extract the final answer.\n",
    "    input_prompt = (\n",
    "        f\"Q: {question}\\n\\n\"\n",
    "        \"Reasoning:\\nLet's think step by step.\\n\\n\"\n",
    "        \"After reasoning, give the final answer on a new line starting with 'A:'.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "    response = client.chat.completions.create(\n",
    "        model='llama3.2:3b',\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    model_resp = response.choices[0].message.content    \n",
    "    answer, reasoning = parse_model_output(model_resp)\n",
    "\n",
    "    print(\"Answer:\", answer)\n",
    "    if reasoning:\n",
    "        print(\"\\nReasoning:\\n\", reasoning)\n",
    "    return answer, reasoning\n",
    "\n",
    "def self_consistent(question, n=10):\n",
    "    # Run multiple reasoning chains and select the most frequent final answer by majority voting.\n",
    "    answer_list, reasoning_list = [], []\n",
    "    for _ in range(n):\n",
    "        ans, reasoning = cot_answer(question)\n",
    "        answer_list.append(ans)\n",
    "        reasoning_list.append(reasoning)\n",
    "    counts = collections.Counter(answer_list)\n",
    "    max_frequency = max(counts.values())\n",
    "    most_voted = [item for item, freq in counts.items() if freq == max_frequency]\n",
    "    return most_voted, counts\n",
    "\n",
    "\n",
    "question = \"What is the square root of 144?\"\n",
    "winner, counter = self_consistent(question)\n",
    "print(\"Votes:\", counter)\n",
    "print(\"Chosen answer:\", winner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bea715",
   "metadata": {},
   "source": [
    "### 2.4: Sequential Revision\n",
    "\n",
    "Sequential revision iteratively improves an answer by generating a first draft, critiquing it, and producing revised drafts that condition on prior answers. Each round should be short and focused, so improvements accumulate without drifting from the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07e5859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem, let's break it down step by step.\n",
      "\n",
      "1. Lydia splits her inheritance into five equal parts, so each part is x/5 of her total inheritance.\n",
      "\n",
      "2. She invests three parts in a high-interest bank account that adds 10% to the value. This means the amount in the bank account becomes (x/3) * 1.10 = (x/3) * 110/100 = (11x/30).\n",
      "\n",
      "3. The rest of her inheritance, which is two parts, plus $500, is invested in the stock market and loses 20%. This means the amount in the stock market becomes ((2x - 500)/5) * 0.80 = (16x - 400)/5.\n",
      "\n",
      "4. Since the two accounts end up with exactly the same amount of money, we can set them equal to each other:\n",
      "\n",
      "(11x/30) = (16x - 400)/5\n",
      "\n",
      "To solve for x, first multiply both sides by 30 to eliminate the fraction on the left side:\n",
      "\n",
      "11x = 6(16x - 400)\n",
      "\n",
      "Expanding and simplifying the right side gives us:\n",
      "\n",
      "11x = 96x - 2400\n",
      "\n",
      "Subtracting 96x from both sides gives us:\n",
      "\n",
      "-85x = -2400\n",
      "\n",
      "Dividing both sides by -85 gives us:\n",
      "\n",
      "x = 28.24 (approximately)\n",
      "\n",
      "Since x represents one of the five equal parts, we multiply it by 5 to find Lydia's total inheritance:\n",
      "\n",
      "28.24 * 5 ‚âà 141.2\n",
      "\n",
      "A: $141.20\n",
      "Step - 0: Answer - $141.20: Reasoning - None\n",
      "To solve this problem, let's break it down step by step.\n",
      "\n",
      "1. Lydia splits her inheritance into five equal parts, so each part is x/5 of her total inheritance.\n",
      "\n",
      "2. She invests three parts in a high-interest bank account that adds 10% to the value. This means the amount in the bank account becomes (x/3) * 1.10 = (x/3) * 110/100 = (11x/30).\n",
      "\n",
      "3. The rest of her inheritance, which is two parts, plus $500, is invested in the stock market and loses 20%. This means the amount in the stock market becomes ((2x - 500)/5) * 0.80 = (16x - 400)/5.\n",
      "\n",
      "4. Since the two accounts end up with exactly the same amount of money, we can set them equal to each other:\n",
      "\n",
      "(11x/30) = (16x - 400)/5\n",
      "\n",
      "To solve for x, first multiply both sides by 30 to eliminate the fraction on the left side:\n",
      "\n",
      "11x = 6(16x - 400)\n",
      "\n",
      "Expanding and simplifying the right side gives us:\n",
      "\n",
      "11x = 96x - 2400\n",
      "\n",
      "Subtracting 96x from both sides gives us:\n",
      "\n",
      "-85x = -2400\n",
      "\n",
      "Dividing both sides by -85 gives us:\n",
      "\n",
      "x = 28.24 (approximately)\n",
      "\n",
      "Since x represents one of the five equal parts, we multiply it by 5 to find Lydia's total inheritance:\n",
      "\n",
      "28.24 * 5 ‚âà 141.2\n",
      "\n",
      "However, let's re-examine our steps and calculations for any potential errors or oversights.\n",
      "\n",
      "Upon reviewing our work, we notice that we multiplied both sides of the equation by 6 when expanding the right side in step 4. This could potentially lead to an incorrect solution.\n",
      "\n",
      "To correct this, we should multiply both sides of the equation by 30 without multiplying the right side by 6:\n",
      "\n",
      "11x = (16x - 400) * 6\n",
      "\n",
      "Expanding and simplifying the right side gives us:\n",
      "\n",
      "11x = 96x - 2400\n",
      "\n",
      "Subtracting 96x from both sides gives us:\n",
      "\n",
      "-85x = -2400\n",
      "\n",
      "Dividing both sides by -85 still gives us:\n",
      "\n",
      "x = 28.24 (approximately)\n",
      "\n",
      "Since x represents one of the five equal parts, we multiply it by 5 to find Lydia's total inheritance:\n",
      "\n",
      "28.24 * 5 ‚âà 141.2\n",
      "\n",
      "However, let's re-evaluate our final answer considering the original problem statement.\n",
      "\n",
      "The correct calculation for the stock market investment is ((2x - 500)/5) * 0.80 = (16x - 400)/5. Multiplying both sides by 5 gives us:\n",
      "\n",
      "(2x - 500) * 0.80 = 16x - 400\n",
      "\n",
      "Expanding and simplifying the left side gives us:\n",
      "\n",
      "1.6x - 400 = 16x - 400\n",
      "\n",
      "Subtracting 1.6x from both sides gives us:\n",
      "\n",
      "-14.4x = 0\n",
      "\n",
      "Dividing both sides by -14.4 does not give a meaningful solution, as it results in x being undefined.\n",
      "\n",
      "Let's re-examine our initial setup and correct the mistake.\n",
      "\n",
      "The correct equation should be:\n",
      "\n",
      "(11x/30) = ((2x - 500)/5) * 0.80\n",
      "\n",
      "Multiplying both sides by 30 to eliminate the fraction on the left side gives us:\n",
      "\n",
      "11x = (6x - 1200) * 4\n",
      "\n",
      "Expanding and simplifying the right side gives us:\n",
      "\n",
      "11x = 24x - 4800\n",
      "\n",
      "Subtracting 24x from both sides gives us:\n",
      "\n",
      "-13x = -4800\n",
      "\n",
      "Dividing both sides by -13 gives us:\n",
      "\n",
      "x = 369.23 (approximately)\n",
      "\n",
      "Since x represents one of the five equal parts, we multiply it by 5 to find Lydia's total inheritance:\n",
      "\n",
      "369.23 * 5 ‚âà 1846.15\n",
      "\n",
      "A: $1846.15\n",
      "Step - 1: Answer - $1846.15: Reasoning - None\n",
      "To solve this problem, let's break it down step by step.\n",
      "\n",
      "1. Lydia splits her inheritance into five equal parts, so each part is x/5 of her total inheritance.\n",
      "\n",
      "2. She invests three parts in a high-interest bank account that adds 10% to the value. This means the amount in the bank account becomes (x/3) * 1.10 = (x/3) * 110/100 = (11x/30).\n",
      "\n",
      "3. The rest of her inheritance, which is two parts, plus $500, is invested in the stock market and loses 20%. This means the amount in the stock market becomes ((2x - 500)/5) * 0.80 = (16x - 400)/5.\n",
      "\n",
      "4. Since the two accounts end up with exactly the same amount of money, we can set them equal to each other:\n",
      "\n",
      "(11x/30) = ((2x - 500)/5) * 0.80\n",
      "\n",
      "Multiplying both sides by 30 to eliminate the fraction on the left side gives us:\n",
      "\n",
      "11x = (6x - 1200) * 4\n",
      "\n",
      "Expanding and simplifying the right side gives us:\n",
      "\n",
      "11x = 24x - 4800\n",
      "\n",
      "Subtracting 24x from both sides gives us:\n",
      "\n",
      "-13x = -4800\n",
      "\n",
      "Dividing both sides by -13 gives us:\n",
      "\n",
      "x = 369.23 (approximately)\n",
      "\n",
      "Since x represents one of the five equal parts, we multiply it by 5 to find Lydia's total inheritance:\n",
      "\n",
      "369.23 * 5 ‚âà 1846.15\n",
      "\n",
      "However, let's re-evaluate our final answer considering the original problem statement.\n",
      "\n",
      "The correct calculation for the stock market investment is ((2x - 500)/5) * 0.80 = (16x - 400)/5. Multiplying both sides by 5 gives us:\n",
      "\n",
      "(2x - 500) * 0.80 = 16x - 400\n",
      "\n",
      "Expanding and simplifying the left side gives us:\n",
      "\n",
      "1.6x - 400 = 16x - 400\n",
      "\n",
      "Subtracting 1.6x from both sides gives us:\n",
      "\n",
      "-14.4x = 0\n",
      "\n",
      "Dividing both sides by -14.4 does not give a meaningful solution, as it results in x being undefined.\n",
      "\n",
      "Let's re-examine our initial setup and correct the mistake.\n",
      "\n",
      "The correct equation should be:\n",
      "\n",
      "(11x/30) = ((2x - 500)/5) * 0.80\n",
      "\n",
      "Multiplying both sides by 30 to eliminate the fraction on the left side gives us:\n",
      "\n",
      "11x = (6x - 1200) * 4\n",
      "\n",
      "Expanding and simplifying the right side gives us:\n",
      "\n",
      "11x = 24x - 4800\n",
      "\n",
      "Subtracting 24x from both sides gives us:\n",
      "\n",
      "-13x = -4800\n",
      "\n",
      "Dividing both sides by -13 gives us:\n",
      "\n",
      "x = 369.23 (approximately)\n",
      "\n",
      "Since x represents one of the five equal parts, we multiply it by 5 to find Lydia's total inheritance:\n",
      "\n",
      "369.23 * 5 ‚âà 1846.15\n",
      "\n",
      "A: $1846.15\n",
      "Step - 2: Answer - $1846.15: Reasoning - None\n",
      "('$1846.15', None)\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def sequential_revision(question: str, max_steps: int = 3) -> str:\n",
    "    # Generate an initial draft answer, then iteratively refine it by conditioning each revision on the previous one.\n",
    "    # Step 1: Ask the model to produce the first draft for the given question\n",
    "    # Step 2: Loop for max_steps-1 times, each time feeding the last draft back to the model with a request to revise\n",
    "    # Step 3: Print each draft to observe how the answer evolves\n",
    "    # Step 4: Return the final improved draft\n",
    "    answer = None\n",
    "    client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "    for step_count in range(max_steps):\n",
    "        if step_count == 0:\n",
    "            input_prompt = (\n",
    "                f\"Q: {question}\\n\\n\"\n",
    "                \"Reasoning:\\nLet's think step by step.\\n\\n\"\n",
    "                \"After reasoning, give the final answer on a new line starting with 'A:'.\"\n",
    "            )\n",
    "        else:\n",
    "            input_prompt = (\n",
    "                f\"Q: {question}\\n\\n\"\n",
    "                f\"In your previous iteration, you generated:\\n'''{model_resp}'''\\n\"\n",
    "                \"Please revise it if needed with step-by-step reasoning.\\n\"\n",
    "                \"Think step by step and reason over the question and the generated answer.\\n\"\n",
    "                \"Reasoning:\\nLet's think step by step.\\n\\n\"\n",
    "                \"After reasoning, give the final answer on a new line starting with 'A:'.\"\n",
    "            )\n",
    "        response = client.chat.completions.create(\n",
    "            model='llama3.2:3b',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": input_prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        model_resp = response.choices[0].message.content\n",
    "        print(model_resp)\n",
    "        answer, reasoning = parse_model_output(model_resp)\n",
    "        print(f\"Step - {step_count}: Answer - {answer}: Reasoning - {reasoning}\")\n",
    "    return answer, reasoning\n",
    "\n",
    "\n",
    "# Step 1: Define a question that benefits from multi-step reasoning\n",
    "# Step 2: Call sequential_revision(question, max_steps)\n",
    "# Step 3: Print the final output\n",
    "question = \"\"\"Lydia inherited a sum of money. \n",
    "She split it into five equal parts. \n",
    "She invested three parts in a high-interest bank account which added 10% to the value. \n",
    "She placed the rest of her inheritance plus $500 in the stock market but lost 20% on that money. \n",
    "If the two accounts end up with exactly the same amount of money in them, how much did she inherit?\n",
    "\"\"\"\n",
    "\n",
    "print(sequential_revision(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9319ee8",
   "metadata": {},
   "source": [
    "### 2.5 Tree‚Äëof‚ÄëThoughts\n",
    "Tree-of-Thoughts reframes reasoning as a search process rather than a single forward chain.\n",
    "Instead of producing one linear sequence of thoughts, the model generates multiple candidate thoughts at each step, evaluates their promise, and then expands only the best few. This allows exploration of different reasoning paths before committing to a final answer, similar to how humans brainstorm, prune, and refine ideas.\n",
    "\n",
    "\n",
    "In this section, you‚Äôll experiment with two simplified versions of ToT:\n",
    "1. Word Ladder puzzle solver: a small example where each ‚Äúthought‚Äù is a candidate word transition.\n",
    "2. Generic ToT search (depth 2, width 2): a minimal logic to expand, evaluate, and select reasoning branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d047801",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Word Ladder Puzzle ##########\n",
    "\n",
    "def neighbors(word, vocabulary):\n",
    "    # Generate all valid one-letter mutations of 'word' that exist in 'vocabulary' and return them.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~6-8 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def tree_of_thought(start, goal, vocab, max_depth=5, beam_width=4):\n",
    "    # Search over partial thoughts (paths) using a small beam.\n",
    "    # Step 1: Initialize the frontier with a single path [start]\n",
    "    # Step 2: For each depth, expand each path by one neighbor from 'neighbors'\n",
    "    # Step 3: Score paths by edit distance between last word and 'goal' (smaller is better)\n",
    "    # Step 4: Keep the top 'beam_width' paths and stop early if any reaches 'goal'\n",
    "    # Step 5: Return the best goal-reaching path or None\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~14-18 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "vocab = {\"hit\",\"dot\",\"cog\",\"log\",\"dog\",\"lot\",\"lit\",\"hot\"}\n",
    "print(tree_of_thought(\"hit\", \"cog\", vocab)) # one candidate solution: ['hit', 'hot', 'dot', 'dog', 'cog']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89067302",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Generic ToT Search ##########\n",
    "\n",
    "import re\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def propose_thoughts(question, state, k=2):\n",
    "    # Propose up to k next ‚Äúthoughts‚Äù that extend the current partial solution/state.\n",
    "    # Steps: build a short prompt with problem + current state; call your client with n=k. Then return a list of stripped strings (‚â§ k).\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~8-10 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def score_state(question, state):\n",
    "    # Score how promising a partial solution is on a 1‚Äì10 scale (higher is better).\n",
    "    # Steps: build a rating prompt; call the model; parse the first integer 1‚Äì10;\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~8-10 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def tree_of_thoughts(question, depth=2, width=2):\n",
    "    # Run a tiny ToT search: expand states with propose_thoughts, score with score_state, keep top-k at each depth.\n",
    "    # Steps: initialize frontier=[(\"\", 0)]; for each depth, expand each state with k=width thoughts; score each; sort by score desc; keep top 'width'; return best state and score.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~12-16 lines)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "question = \"Design a plan for a weekend science workshop for 12-year-olds.\"\n",
    "solution, score = tree_of_thoughts(question)\n",
    "\n",
    "print(f\"Best solution (score {score}):\\n{solution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc38f6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 3‚Äë Training Models for Reasoning\n",
    "\n",
    "### 3.1: CoT Training\n",
    "Chain-of-Thought (CoT) training conditions the model on explicit rationales during fine-tuning. Instead of teaching the model to output only the final answer, we train on (question, rationale, answer) so the model learns to internalize multi-step reasoning patterns. A practical recipe is STaR (Self-Taught Reasoner), which uses a stronger teacher model to bootstrap rationales that a smaller student can learn from.\n",
    "\n",
    "For tasks that require multi-hop reasoning, models fine-tuned on rationales often achieve higher accuracy and are more stable at inference time than models trained on direct answers only. \n",
    "\n",
    "Training a full language model is beyond the scope of this notebook, but here is the high-level workflow followed by a short pseudocode:\n",
    "- Collect questions: Prepare a dataset of questions and correct answers.\n",
    "- Generate rationales: Use a strong LLM to produce step-by-step reasoning ending with the correct answer.\n",
    "- Filter and clean: Discard incorrect or low-quality rationales.\n",
    "- Prepare training data: Format triples (question, rationale, answer) for supervised fine-tuning.\n",
    "- Fine-tune: Fine-tune the LLM on rationales.\n",
    "- Iterate: Refine prompts, improve data quality, and retrain for stronger reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode (STaR loop)\n",
    "# for round in 1 ... iters:\n",
    "    # STEP 1: self-generate reasoning (teacher creates rationale + answer)\n",
    "    # STEP 2: keep only correct, high-quality traces\n",
    "    # STEP 3: fine-tune student on (question, rationale, answer) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b53c70",
   "metadata": {},
   "source": [
    "### 3.2: ORM¬†vs¬†PRM¬†+ RL\n",
    "Training a Reward Model (RM) allows large language models to be improved through reinforcement learning (RL). Instead of fine-tuning directly on examples, we train a separate model that can score or rank model outputs, and use those scores as feedback signals to refine the policy model.\n",
    "\n",
    "Two main reward modeling approaches are ORM (predicts a scalar reward for the final answer) and PRM (evaluates the reasoning steps instead of just the outcome)\n",
    "\n",
    "\n",
    "\n",
    "| Approach | Typical loss | When to use |\n",
    "|-----------|-------------|-------------|\n",
    "|*Outcome Reward Model* | Predict scalar reward | Easy to collect training data using verifiers |\n",
    "|*Process Reward Model* | Predict rewards per step | Difficult to collect training data but more accurate |\n",
    "| *RLHF* | Use RM as reward in **RL** fine‚Äëtuning | Aligns policy with human signals | Aligns model policy with human or synthetic preferences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for round = 1 ... iters:\n",
    "    # STEP 1:  Generate reasoning\n",
    "        # sample a minibatch of questions\n",
    "        # policy roll‚Äëout (actions + log‚Äëprobs)\n",
    "    # STEP 2:  Score the trajectory\n",
    "        # ORM: scalar reward for the final answer / PRM: scalar reward for the thought process\n",
    "    # STEP 3:  Reinforce the policy (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a81a6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 4‚Äë A Deep Research Agent\n",
    "\n",
    "A deep-research agent pairs a reasoning model (e.g., deepseek-r1) with external tools for web search and retrieval. We will follow the ReAct pattern: the model writes short thoughts, decides when to call tools, reads observations, and continues reasoning until it can answer or reaches a step limit.\n",
    "\n",
    "We now combine a **search tool** with a reasoning model (e.g., `deepseek-r1`) in a multi-step setup. We follow the *ReAct* pattern (reason ‚Üí tool ‚Üí observation):\n",
    "\n",
    "1. The model reasoins and decides to use tools\n",
    "2. The agent searches and feed condensed snippets back as context\n",
    "3. Iterate until the model answers or hits a step limit\n",
    "\n",
    "We use `AgentType.OPENAI_FUNCTIONS`, which hides the loop inside the LangChain agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd1e1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from langchain.tools import Tool\n",
    "\n",
    "def ddg_search(query: str, k: int = 5) -> str:\n",
    "    # Use DDGS to run a simple web search and return joined snippets.\n",
    "    with DDGS() as ddgs:\n",
    "        results = [hit[\"body\"] for hit in ddgs.text(query, max_results=k)]\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"DuckDuckGo Search\",\n",
    "    func=ddg_search,\n",
    "    description=\"Search the public web. Input: a plain English query. Returns: concatenated snippets.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "418a0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the best resources for learning machine learning in 2025, I need to search for relevant information on the web.\n",
      "\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: \"best machine learning resources 2025\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIf you take Andrew Ng‚Äôs Machine Learning course, which uses Octave, you should learn Python either during the course or after since you‚Äôll need it eventually. Additionally, another excellent Python resource is dataquest.io, which has many free Python lessons in their interactive browser environment.\n",
      "December 12, 2024 - Welcome to r/learnmachinelearning - a community of learners and educators passionate about machine learning! This is your space to ask questions, share resources, and grow together in understanding ML concepts - from basic principles to advanced techniques. Whether you're writing your first neural network or diving into transformers, you'll find supportive peers here. For ML research, /r/machinelearning For resume review, /r/engineeringresumes For ML engineers, /r/mlengineering ... Accessibility Reddit, Inc. ¬© 2025.\n",
      "July 16, 2025 - Keras is best for quickly prototyping and experimenting with neural network architectures. Getting familiar with machine learning software can seem daunting, but there are many resources that can help you get started and progress in your journey.\n",
      "March 3, 2025 - Explore top-notch 50+ Machine Learning resources for self-study in 2025. From courses to tutorials, find everything you need today!\n",
      "April 20, 2025 - PyTorch for Deep Learning & Machine Learning ‚Äì Full Course ‚Äì YouTube ... TensorFlow Tutorial 1 ‚Äì Installation and Setup Deep Learning Environment (Anaconda and PyCharm ) (Recommended) TensorFlow 2.0 Complete Course ‚Äì Python Neural Networks ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: What are the best resources to learn machine learning in 2025?\n",
      "Thought: I now know the final answer\n",
      "Final Answer: Based on the search results, some of the best resources for learning machine learning in 2025 include Andrew Ng's Machine Learning course, dataquest.io, and Keras. Additionally, there are many other resources available, such as courses, tutorials, and YouTube channels like PyTorch for Deep Learning & Machine Learning and TensorFlow Tutorial 1.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What are the best resources to learn machine learning in 2025?',\n",
       " 'output': \"Based on the search results, some of the best resources for learning machine learning in 2025 include Andrew Ng's Machine Learning course, dataquest.io, and Keras. Additionally, there are many other resources available, such as courses, tutorials, and YouTube channels like PyTorch for Deep Learning & Machine Learning and TensorFlow Tutorial 1.\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Running deepseek 8b sucks on MBA m1 8GB\n",
    "MODEL = \"llama3.2:3b\"\n",
    "question = \"What are the best resources to learn machine learning in 2025?\"\n",
    "\n",
    "# Step 1: Initialize the reasoning model via ChatOllama\n",
    "llm = ChatOllama(\n",
    "    model = MODEL,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Step 2: Build the agent with tool access (DuckDuckGo Search) and function-calling interface (initialize_agent)\n",
    "agent_tools = [search_tool]\n",
    "agent = initialize_agent(agent_tools, llm, type=AgentType.OPENAI_FUNCTIONS, verbose = True)\n",
    "\n",
    "\n",
    "# Step 3: Ask a query and let the agent search + reason to produce an answer\n",
    "agent.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1c3f7",
   "metadata": {},
   "source": [
    "# Optional (Multi-agent Deep Research)\n",
    "Instead of a single multi-step agent, you can design multiple collaborating agents such as a Planner, Searcher, Summarizer, and Verifier that pass information and refine each other‚Äôs outputs. This setup improves robustness, diversity of reasoning, and division of labor.\n",
    "\n",
    "Try building a simple setup with 2‚Äì3 agents that share goals and messages, for example Planner ‚Üí Researcher ‚Üí Writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59abf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_research(query, n=3):\n",
    "    # Run n independent research runs in parallel and return their answers.\n",
    "    # Steps: use ThreadPoolExecutor; submit n calls to your agent/search pipeline; gather results in order.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE\n",
    "    \"\"\"\n",
    "\n",
    "answers = parallel_research(\"What are the best resources to learn ML in 2025?\")\n",
    "for i,a in enumerate(answers,1):\n",
    "    print(f\"[Run {i}] {a[:200]}‚Ä¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507d0a4",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "* Practised various inference‚Äëtime reasoning methods\n",
    "* Gained intuition about training reasoning models\n",
    "* You have built a **deep-research agent**: reasoning model like deep-seek r1 + ReAct-style agent + tool use (web search)\n",
    "* Try adding more tools, and extending the deep-research to a multi-agent system: many agents researching web in parallel.\n",
    "\n",
    "\n",
    "üëè **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
